{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "pycharm": {
      "stem_cell": {
        "cell_type": "raw",
        "source": [],
        "metadata": {
          "collapsed": false
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# End of Term project\n",
        "## Running SpeechBrain-MOABB data with a Simpler Model, SimpleEEGNEt\n",
        "Reasearch Question: Can a model with a simpler architecture achieve comparable accuracy to EEGNet in a shorter training time? In response to this question, I developed a new model named SimpleEEGNet.<br>\n",
        "\"EEGNet vs. SimpleEEGNet Comparison:\n",
        "\n",
        "Architecture:<br>\n",
        "* EEGNet: Complex, utilizing various convolution types and pooling.<br>\n",
        "* SimpleEEGNet: Simplified, featuring two convolutional layers and average pooling.<br>\n",
        "\n",
        "Fully Connected Layers:<br>\n",
        "* EEGNet: Single layer.<br>\n",
        "* SimpleEEGNet: Three layers, offering more depth.<br>\n",
        "\n",
        "Dropout:<br>\n",
        "* EEGNet: After each pooling layer.<br>\n",
        "* SimpleEEGNet: After the second convolutional layer.<br>\n",
        "\n",
        "Implementation:<br>\n",
        "* EEGNet: SpeechBrain library.<br>\n",
        "* SimpleEEGNet: Direct PyTorch implementation.<br>\n",
        "\n",
        "Activation Functions:<br>\n",
        "* Both models support various functions.<br>\n",
        "\n",
        "Output Layer:<br>\n",
        "* Both use LogSoftmax for classification.<br>"
      ],
      "metadata": {
        "id": "gxG3UuKZrsW6"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JnRSS3jGxVsk"
      },
      "source": [
        "## **Prerequisites**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Download SpeechBrain-MOABB\n",
        "\n",
        "SpeechBrain-MOABB can be downloaded from the GitHub repository listed below."
      ],
      "metadata": {
        "id": "-GhrvkOFT9_t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!git clone https://github.com/speechbrain/benchmarks.git\n",
        "%cd benchmarks\n",
        "!git submodule update --init --recursive\n",
        "%cd speechbrain\n",
        "!pip install -r requirements.txt\n",
        "!pip install -e .\n",
        "%cd /content/benchmarks/benchmarks/MOABB\n",
        "!pip install -r ../../requirements.txt    # Install base dependencies\n",
        "!pip install -r extra-requirements.txt    # Install additional dependencies\n",
        "%cd /content/benchmarks/benchmarks/MOABB\n",
        "%env PYTHON_PATH=/content/benchmarks/"
      ],
      "metadata": {
        "id": "ztkQvyGGE_si"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Save my own implemented model and its yaml file\n",
        "\n",
        "Run the two next cells below to save my own implemented model(SimpleEEGNet.py) in /content/benchmarks/benchmarks/MOABB/models/<br>\n",
        "and its yaml file (SimpleEEGNet.yaml) in /content/benchmarks/benchmarks/MOABB/hparams/MotorImagery/BNCI2014001/"
      ],
      "metadata": {
        "id": "XstC_wLmj8aS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#CREATE SimpleEEGNet.yaml file\n",
        "content = \"\"\"\n",
        "seed: 1234\n",
        "__set_torchseed: !apply:torch.manual_seed [!ref <seed>]\n",
        "\n",
        "# DIRECTORIES\n",
        "data_folder: !PLACEHOLDER  #'/path/to/dataset'. The dataset will be automatically downloaded in this folder\n",
        "cached_data_folder: !PLACEHOLDER #'path/to/pickled/dataset'\n",
        "output_folder: !PLACEHOLDER #'path/to/results'\n",
        "\n",
        "# DATASET HPARS\n",
        "# Defining the MOABB dataset.\n",
        "dataset: !new:moabb.datasets.BNCI2014001\n",
        "save_prepared_dataset: True\n",
        "data_iterator_name: 'leave-one-session-out'\n",
        "target_subject_idx: 0\n",
        "target_session_idx: 1\n",
        "events_to_load: null\n",
        "original_sample_rate: 250\n",
        "sample_rate: 125\n",
        "fmin: 1\n",
        "fmax: 40\n",
        "n_classes: 4\n",
        "tmin: 0.\n",
        "tmax: 4.0\n",
        "n_steps_channel_selection: 3\n",
        "T: !apply:math.ceil\n",
        "    - !ref <sample_rate> * (<tmax> - <tmin>)\n",
        "C: 22\n",
        "test_with: 'best'\n",
        "test_key: \"acc\"\n",
        "\n",
        "# METRICS\n",
        "f1: !name:sklearn.metrics.f1_score\n",
        "    average: 'macro'\n",
        "acc: !name:sklearn.metrics.balanced_accuracy_score\n",
        "cm: !name:sklearn.metrics.confusion_matrix\n",
        "metrics:\n",
        "    f1: !ref <f1>\n",
        "    acc: !ref <acc>\n",
        "    cm: !ref <cm>\n",
        "\n",
        "# TRAINING HPARS\n",
        "n_train_examples: 100\n",
        "avg_models: 10\n",
        "number_of_epochs: 1000\n",
        "lr: 0.0001\n",
        "max_lr: !ref <lr>\n",
        "base_lr: 0.00000001\n",
        "step_size_multiplier: 5\n",
        "step_size: !apply:round\n",
        "    - !ref <step_size_multiplier> * <n_train_examples> / <batch_size>\n",
        "lr_annealing: !new:speechbrain.nnet.schedulers.CyclicLRScheduler\n",
        "    base_lr: !ref <base_lr>\n",
        "    max_lr: !ref <max_lr>\n",
        "    step_size: !ref <step_size>\n",
        "label_smoothing: 0.0\n",
        "loss: !name:speechbrain.nnet.losses.nll_loss\n",
        "    label_smoothing: !ref <label_smoothing>\n",
        "optimizer: !name:torch.optim.Adam\n",
        "    lr: !ref <lr>\n",
        "epoch_counter: !new:speechbrain.utils.epoch_loop.EpochCounter\n",
        "    limit: !ref <number_of_epochs>\n",
        "batch_size: 32\n",
        "valid_ratio: 0.2\n",
        "\n",
        "# DATA AUGMENTATION\n",
        "# cutcat (disabled when min_num_segments=max_num_segments=1)\n",
        "max_num_segments: 3 # @orion_step2: --max_num_segments~\"uniform(2, 6, discrete=True)\"\n",
        "cutcat: !new:speechbrain.augment.time_domain.CutCat\n",
        "    min_num_segments: 2\n",
        "    max_num_segments: !ref <max_num_segments>\n",
        "# random amplitude gain between 0.5-1.5 uV (disabled when amp_delta=0.)\n",
        "amp_delta: 0.01742 # @orion_step2: --amp_delta~\"uniform(0.0, 0.5)\"\n",
        "rand_amp: !new:speechbrain.augment.time_domain.RandAmp\n",
        "    amp_low: !ref 1 - <amp_delta>\n",
        "    amp_high: !ref 1 + <amp_delta>\n",
        "# random shifts between -300 ms to 300 ms (disabled when shift_delta=0.)\n",
        "shift_delta_: 1 # orion_step2: --shift_delta_~\"uniform(0, 25, discrete=True)\"\n",
        "shift_delta: !ref 1e-2 * <shift_delta_> # 0.250 # 0.-0.25 with steps of 0.01\n",
        "min_shift: !apply:math.floor\n",
        "    - !ref 0 - <sample_rate> * <shift_delta>\n",
        "max_shift: !apply:math.floor\n",
        "    - !ref 0 + <sample_rate> * <shift_delta>\n",
        "time_shift: !new:speechbrain.augment.freq_domain.RandomShift\n",
        "    min_shift: !ref <min_shift>\n",
        "    max_shift: !ref <max_shift>\n",
        "    dim: 1\n",
        "# injection of gaussian white noise\n",
        "snr_white_low: 15.0 # @orion_step2: --snr_white_low~\"uniform(0.0, 15, precision=2)\"\n",
        "snr_white_delta: 19.1 # @orion_step2: --snr_white_delta~\"uniform(5.0, 20.0, precision=3)\"\n",
        "snr_white_high: !ref <snr_white_low> + <snr_white_delta>\n",
        "add_noise_white: !new:speechbrain.augment.time_domain.AddNoise\n",
        "    snr_low: !ref <snr_white_low>\n",
        "    snr_high: !ref <snr_white_high>\n",
        "\n",
        "repeat_augment: 1 # @orion_step1: --repeat_augment 0\n",
        "augment: !new:speechbrain.augment.augmenter.Augmenter\n",
        "    parallel_augment: True\n",
        "    concat_original: True\n",
        "    parallel_augment_fixed_bs: True\n",
        "    repeat_augment: !ref <repeat_augment>\n",
        "    shuffle_augmentations: True\n",
        "    min_augmentations: 4\n",
        "    max_augmentations: 4\n",
        "    augmentations: [\n",
        "        !ref <cutcat>,\n",
        "        !ref <rand_amp>,\n",
        "        !ref <time_shift>,\n",
        "        !ref <add_noise_white>]\n",
        "\n",
        "# DATA NORMALIZATION\n",
        "dims_to_normalize: 1\n",
        "normalize: !name:speechbrain.processing.signal_processing.mean_std_norm\n",
        "    dims: !ref <dims_to_normalize>\n",
        "\n",
        "# MODEL\n",
        "input_shape: [null, !ref <T>, !ref <C>, null]\n",
        "cnn_temporal_kernels: 4\n",
        "cnn_temporal_kernelsize: 20\n",
        "cnn_spatial_depth_multiplier: 1\n",
        "cnn_spatial_pool: 2\n",
        "activation_type: 'relu'\n",
        "\n",
        "model: !new:models.SimpleEEGNet.SimpleEEGNet\n",
        "    input_shape: !ref <input_shape>\n",
        "    cnn_temporal_kernels: !ref <cnn_temporal_kernels>\n",
        "    cnn_temporal_kernelsize: [!ref <cnn_temporal_kernelsize>, 1]\n",
        "    cnn_spatial_depth_multiplier: !ref <cnn_spatial_depth_multiplier>\n",
        "    cnn_spatial_pool: !ref <cnn_spatial_pool>\n",
        "    activation_type: !ref <activation_type>\n",
        "    dense_n_neurons: !ref <n_classes>\n",
        "\"\"\"\n",
        "with open('/content/benchmarks/benchmarks/MOABB/hparams/MotorImagery/BNCI2014001/SimpleEEGNet.yaml', 'w') as f:\n",
        "    f.write(content)"
      ],
      "metadata": {
        "id": "uYjLTjN6hHBo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#CREATE SimpleEEGNet.py file\n",
        "content = \"\"\"\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class SimpleEEGNet(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_shape=None,  # (1, T, C, 1)\n",
        "        cnn_temporal_kernels=8,\n",
        "        cnn_temporal_kernelsize=32,\n",
        "        cnn_spatial_depth_multiplier=2,\n",
        "        cnn_spatial_pool=4,\n",
        "        dropout=0.5,\n",
        "        dense_n_neurons=4,\n",
        "        activation_type=\"elu\",\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        if input_shape is None:\n",
        "            raise ValueError(\"Must specify input_shape\")\n",
        "\n",
        "        if activation_type == \"gelu\":\n",
        "            activation = nn.GELU()\n",
        "        elif activation_type == \"elu\":\n",
        "            activation = nn.ELU()\n",
        "        elif activation_type == \"relu\":\n",
        "            activation = nn.ReLU()\n",
        "        elif activation_type == \"leaky_relu\":\n",
        "            activation = nn.LeakyReLU()\n",
        "        elif activation_type == \"prelu\":\n",
        "            activation = nn.PReLU()\n",
        "        else:\n",
        "            raise ValueError(\"Wrong hidden activation function\")\n",
        "\n",
        "        # Define CNN layers\n",
        "        self.conv1 = nn.Conv2d(1, cnn_temporal_kernels, kernel_size=(cnn_temporal_kernelsize, 1))\n",
        "        self.bn1 = nn.BatchNorm2d(cnn_temporal_kernels)\n",
        "        self.conv2 = nn.Conv2d(cnn_temporal_kernels, cnn_spatial_depth_multiplier * cnn_temporal_kernels, kernel_size=(1, input_shape[2]))\n",
        "        self.bn2 = nn.BatchNorm2d(cnn_spatial_depth_multiplier * cnn_temporal_kernels)\n",
        "        self.activation = activation\n",
        "        self.pool = nn.AvgPool2d(kernel_size=(cnn_spatial_pool, 1))\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        # Calculate output shape after convolutional layers\n",
        "        with torch.no_grad():\n",
        "            input = torch.ones((1,) + tuple(input_shape[1:-1]) + (1,))\n",
        "            output = self.forward_conv(input)\n",
        "            self.num_flat_features = output.view(-1).size(0)\n",
        "\n",
        "        # Define fully connected layers\n",
        "        self.fc1 = nn.Linear(self.num_flat_features, dense_n_neurons)\n",
        "        self.fc2 = nn.Linear(dense_n_neurons, dense_n_neurons)\n",
        "        self.fc3 = nn.Linear(dense_n_neurons, dense_n_neurons)\n",
        "        self.fc_out = nn.Linear(dense_n_neurons, dense_n_neurons)\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward_conv(self, x):\n",
        "        x = x.permute(0, 3, 2, 1)\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.activation(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.bn2(x)\n",
        "        x = self.activation(x)\n",
        "        x = self.pool(x)\n",
        "        x = self.dropout(x)\n",
        "        return x\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.forward_conv(x)\n",
        "        x = x.view(-1, self.num_flat_features)\n",
        "        x = self.fc1(x)\n",
        "        x = self.activation(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.activation(x)\n",
        "        x = self.fc3(x)\n",
        "        x = self.activation(x)\n",
        "        x = self.fc_out(x)\n",
        "        x = self.softmax(x)\n",
        "        return x\n",
        "\"\"\"\n",
        "with open('/content/benchmarks/benchmarks/MOABB/models/SimpleEEGNet.py', 'w') as f:\n",
        "    f.write(content)"
      ],
      "metadata": {
        "id": "lSHf7EYdit9M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Define a yaml file containing the hyper-parameters defining a decoding pipeline**\n",
        "\n",
        "Let us address 4-class motor imagery decoding using BNCI2014-001 dataset (also known as \"BCI IV2a dataset\"), by adopting a leave-one-session-out strategy using the first participants' signals, leaving out the session named '0test'. EEGNet is used as decoder.\n",
        "\n",
        "You can set all hyper-parameters to specific values if you already know them; otherwise, you can set them to placeholders (i.e., as `!PLACEHOLDER`). For example, folders (e.g., the data folder, the folder for compressed dataset, and the output folder), and dataset information (e.g., data iterator, index of subject and session to use) are usually kept as placeholders.\n",
        "\n",
        "Before start writing the yaml file, please follow the SpeechBrain tutorial dedicated to HyperPyYAML at https://speechbrain.github.io/tutorial_basics.html."
      ],
      "metadata": {
        "id": "f45uzf3mUNyc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from hyperpyyaml import load_hyperpyyaml, dump_hyperpyyaml\n",
        "\n",
        "example_hyperparams = \"\"\"\n",
        "seed: 1234\n",
        "__set_torchseed: !apply:torch.manual_seed [!ref <seed>]\n",
        "\n",
        "# DIRECTORIES\n",
        "data_folder: !PLACEHOLDER  #'/path/to/dataset'. The dataset will be automatically downloaded in this folder\n",
        "cached_data_folder: !PLACEHOLDER #'path/to/pickled/dataset'\n",
        "output_folder: !PLACEHOLDER #'path/to/results'\n",
        "\n",
        "# DATASET HPARS\n",
        "# Defining the MOABB dataset.\n",
        "dataset: !new:moabb.datasets.BNCI2014001\n",
        "save_prepared_dataset: True\n",
        "data_iterator_name: 'leave-one-session-out'\n",
        "target_subject_idx: 0\n",
        "target_session_idx: 1\n",
        "events_to_load: null\n",
        "original_sample_rate: 250\n",
        "sample_rate: 125\n",
        "fmin: 3.6\n",
        "fmax: 31.1\n",
        "n_classes: 4\n",
        "tmin: 0.\n",
        "tmax: 4.0\n",
        "n_steps_channel_selection: 3\n",
        "T: !apply:math.ceil\n",
        "    - !ref <sample_rate> * (<tmax> - <tmin>)\n",
        "C: 22\n",
        "test_with: 'best'\n",
        "test_key: \"acc\"\n",
        "\n",
        "# METRICS\n",
        "f1: !name:sklearn.metrics.f1_score\n",
        "    average: 'macro'\n",
        "acc: !name:sklearn.metrics.balanced_accuracy_score\n",
        "cm: !name:sklearn.metrics.confusion_matrix\n",
        "metrics:\n",
        "    f1: !ref <f1>\n",
        "    acc: !ref <acc>\n",
        "    cm: !ref <cm>\n",
        "\n",
        "# TRAINING HPARS\n",
        "n_train_examples: 100\n",
        "avg_models: 10\n",
        "number_of_epochs: 900\n",
        "lr: lr: 0.005\n",
        "max_lr: !ref <lr>\n",
        "base_lr: 0.00000001\n",
        "step_size_multiplier: 5\n",
        "step_size: !apply:round\n",
        "    - !ref <step_size_multiplier> * <n_train_examples> / <batch_size>\n",
        "lr_annealing: !new:speechbrain.nnet.schedulers.CyclicLRScheduler\n",
        "    base_lr: !ref <base_lr>\n",
        "    max_lr: !ref <max_lr>\n",
        "    step_size: !ref <step_size>\n",
        "label_smoothing: 0.0\n",
        "loss: !name:speechbrain.nnet.losses.nll_loss\n",
        "    label_smoothing: !ref <label_smoothing>\n",
        "optimizer: !name:torch.optim.Adam\n",
        "    lr: !ref <lr>\n",
        "epoch_counter: !new:speechbrain.utils.epoch_loop.EpochCounter\n",
        "    limit: !ref <number_of_epochs>\n",
        "batch_size: 32\n",
        "valid_ratio: 0.2\n",
        "\n",
        "# DATA NORMALIZATION\n",
        "dims_to_normalize: 1\n",
        "normalize: !name:speechbrain.processing.signal_processing.mean_std_norm\n",
        "    dims: !ref <dims_to_normalize>\n",
        "\n",
        "# MODEL\n",
        "input_shape: [null, !ref <T>, !ref <C>, null]\n",
        "cnn_temporal_kernels: 4\n",
        "cnn_temporal_kernelsize: 20\n",
        "cnn_spatial_depth_multiplier: 1\n",
        "cnn_spatial_pool: 2\n",
        "activation_type: 'relu'\n",
        "\n",
        "model: !new:models.SimpleEEGNet.SimpleEEGNet\n",
        "    input_shape: !ref <input_shape>\n",
        "    cnn_temporal_kernels: !ref <cnn_temporal_kernels>\n",
        "    cnn_temporal_kernelsize: !ref <cnn_temporal_kernelsize>\n",
        "    cnn_spatial_depth_multiplier: !ref <cnn_spatial_depth_multiplier>\n",
        "    cnn_spatial_pool: !ref <cnn_spatial_pool>\n",
        "    activation_type: !ref <activation_type>\n",
        "    dense_n_neurons: !ref <n_classes>\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# Save the yaml file on disk\n",
        "f = open('/content/example_hyperparams.yaml', \"w\")\n",
        "f.write(example_hyperparams)\n",
        "f.close()"
      ],
      "metadata": {
        "id": "f_m7D3eiUbHC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Note about data augmentation**\n",
        "\n",
        "It is worth highlighting that in the previous yaml file, no data augmentation was included. However, you can easily add data augmentation by defining each augmenter (e.g., applying CutCat and random time shift).\n",
        "\n",
        "The so-defined augmenters are provided as input to the `Augmenter` class, that will combine and apply the augmenters. For instance, you can perform the augmenters in sequence or in parallel (`parallel_augment` input parameter), use one or more augmenters for augmenting each mini-batch of data (`min_augmentations` and `max_augmentations` input parameters), and repeat data augmentation multiple times for each mini-batch (`repeat_augment` input parameter). See `Augmenter` documentation for further details."
      ],
      "metadata": {
        "id": "mGsMSB2RvZX5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_augmentation_hyperparams = \"\"\"\n",
        "# DATA AUGMENTATION\n",
        "# cutcat (disabled when min_num_segments=max_num_segments=1)\n",
        "max_num_segments: 3 # @orion_step2: --max_num_segments~\"uniform(2, 6, discrete=True)\"\n",
        "cutcat: !new:speechbrain.augment.time_domain.CutCat\n",
        "    min_num_segments: 2\n",
        "    max_num_segments: !ref <max_num_segments>\n",
        "# random amplitude gain between 0.5-1.5 uV (disabled when amp_delta=0.)\n",
        "amp_delta: 0.01742 # @orion_step2: --amp_delta~\"uniform(0.0, 0.5)\"\n",
        "rand_amp: !new:speechbrain.augment.time_domain.RandAmp\n",
        "    amp_low: !ref 1 - <amp_delta>\n",
        "    amp_high: !ref 1 + <amp_delta>\n",
        "# random shifts between -300 ms to 300 ms (disabled when shift_delta=0.)\n",
        "shift_delta_: 1 # orion_step2: --shift_delta_~\"uniform(0, 25, discrete=True)\"\n",
        "shift_delta: !ref 1e-2 * <shift_delta_> # 0.250 # 0.-0.25 with steps of 0.01\n",
        "min_shift: !apply:math.floor\n",
        "    - !ref 0 - <sample_rate> * <shift_delta>\n",
        "max_shift: !apply:math.floor\n",
        "    - !ref 0 + <sample_rate> * <shift_delta>\n",
        "time_shift: !new:speechbrain.augment.freq_domain.RandomShift\n",
        "    min_shift: !ref <min_shift>\n",
        "    max_shift: !ref <max_shift>\n",
        "    dim: 1\n",
        "# injection of gaussian white noise\n",
        "snr_white_low: 15.0 # @orion_step2: --snr_white_low~\"uniform(0.0, 15, precision=2)\"\n",
        "snr_white_delta: 19.1 # @orion_step2: --snr_white_delta~\"uniform(5.0, 20.0, precision=3)\"\n",
        "snr_white_high: !ref <snr_white_low> + <snr_white_delta>\n",
        "add_noise_white: !new:speechbrain.augment.time_domain.AddNoise\n",
        "    snr_low: !ref <snr_white_low>\n",
        "    snr_high: !ref <snr_white_high>\n",
        "\n",
        "repeat_augment: 1 # @orion_step1: --repeat_augment 0\n",
        "augment: !new:speechbrain.augment.augmenter.Augmenter\n",
        "    parallel_augment: True\n",
        "    concat_original: True\n",
        "    parallel_augment_fixed_bs: True\n",
        "    repeat_augment: !ref <repeat_augment>\n",
        "    shuffle_augmentations: True\n",
        "    min_augmentations: 4\n",
        "    max_augmentations: 4\n",
        "    augmentations: [\n",
        "        !ref <cutcat>,\n",
        "        !ref <rand_amp>,\n",
        "        !ref <time_shift>,\n",
        "        !ref <add_noise_white>]\n",
        "\"\"\"\n",
        "\n",
        "example_hyperparams += data_augmentation_hyperparams"
      ],
      "metadata": {
        "id": "PtMD8LoevYg4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Train the neural network on a single cross-validation fold**\n",
        "\n",
        "Start network training by running the `train.py` script providing the filepath to the yaml file and by overriding the variables set as placeholders in the yaml file. Furthermore, here for brevity we override also the number of training epochs to 50 epochs (instead of 1000 epochs) with `--number_of_epochs 50`. It is worth highlighting that you can also override any other hyper-parameters in the same way."
      ],
      "metadata": {
        "id": "qiOA1AlVg48b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "# Start time\n",
        "start_time = time.time()\n",
        "\n",
        "%cd /content/benchmarks/benchmarks/MOABB/\n",
        "\n",
        "!python train.py /content/example_hyperparams.yaml \\\n",
        "--data_folder '/content/data/BNCI2014001' \\\n",
        "--cached_data_folder '/content/data' \\\n",
        "--output_folder '/content/results/single-fold-example/BNCI2014001' \\\n",
        "--data_iterator_name 'leave-one-session-out' \\\n",
        "--target_subject_idx 0 \\\n",
        "--target_session_idx 1 \\\n",
        "--number_of_epochs 50 \\\n",
        "--device 'cpu' # Switch to cuda for a speed up.\n",
        "\n",
        "# End time\n",
        "end_time = time.time()\n",
        "# Calculate elapsed time\n",
        "elapsed_time = end_time - start_time\n",
        "# Print the elapsed time\n",
        "print(\"Elapsed time: {:.4f} seconds\".format(elapsed_time))"
      ],
      "metadata": {
        "id": "pYEa8oubbvk-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Run a complete experiment by looping over the entire dataset**\n",
        "\n",
        "In the previous cell, `train.py` was called for a single cross-validation fold (e.g., one participant and one held-out session in leave-one-session-out cross-validation). Thus, we provide a command line interface for easily running training on all participants and cross-validation folds (using `./run_experiments.sh`).\n",
        "\n",
        "Here, besides the relevant folders, you should specify the hyper-parameter file, the number of participants and sessions to use, the data iteration scheme (leave-one-session-out or leave-one-subject-out). In addition, you can also run the code multiple times, each time with a different random seed used for initializing weights (by setting the `nruns` parameters). Finally, you can define the `device` to use (set to `cpu` if you do not have a GPU).\n",
        "\n"
      ],
      "metadata": {
        "id": "ITKUTG0JUcha"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Start time\n",
        "start_time = time.time()\n",
        "\n",
        "!./run_experiments.sh --hparams /content/example_hyperparams.yaml \\\n",
        "--data_folder '/content/data/BNCI2014001'\\\n",
        "--cached_data_folder '/content/data' \\\n",
        "--output_folder '/content/results/full-experiment/BNCI2014001' \\\n",
        "--nsbj 9 --nsess 2 --nruns 1 --train_mode 'leave-one-session-out' \\\n",
        "--number_of_epochs 50 \\\n",
        "--device 'cpu'\n",
        "\n",
        "# End time\n",
        "end_time = time.time()\n",
        "# Calculate elapsed time\n",
        "elapsed_time = end_time - start_time\n",
        "# Print the elapsed time\n",
        "print(\"Elapsed time: {:.4f} seconds\".format(elapsed_time))"
      ],
      "metadata": {
        "id": "mOD1JbH2TSZj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "pNYi3lQdIgM5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tuned_hyperparams = \"\"\"\n",
        "# DATASET HPARS\n",
        "fmin: 1  # @orion_step1: --fmin~\"uniform(0.1, 5, precision=2)\"\n",
        "fmax: 40  # @orion_step1: --fmax~\"uniform(20.0, 50.0, precision=3)\"\n",
        "\n",
        "# TRAINING HPARS\n",
        "number_of_epochs: 1000  # @orion_step1: --number_of_epochs~\"uniform(50, 200, discrete=True)\"\n",
        "lr: 0.0001  # @orion_step1: --lr~\"choices([0.01, 0.005, 0.001, 0.0005, 0.0001])\"\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "other_hyperparams = \"\"\"\n",
        "seed: 1234\n",
        "__set_torchseed: !apply:torch.manual_seed [!ref <seed>]\n",
        "\n",
        "# DIRECTORIES\n",
        "data_folder: !PLACEHOLDER  #'/path/to/dataset'. The dataset will be automatically downloaded in this folder\n",
        "cached_data_folder: !PLACEHOLDER #'path/to/pickled/dataset'\n",
        "output_folder: !PLACEHOLDER #'path/to/results'\n",
        "\n",
        "# DATASET HPARS\n",
        "# Defining the MOABB dataset.\n",
        "dataset: !new:moabb.datasets.BNCI2014001\n",
        "save_prepared_dataset: True\n",
        "data_iterator_name: 'leave-one-session-out'\n",
        "target_subject_idx: 0\n",
        "target_session_idx: 1\n",
        "events_to_load: null\n",
        "original_sample_rate: 250\n",
        "sample_rate: 125\n",
        "n_classes: 4\n",
        "tmin: 0.\n",
        "tmax: 4.0\n",
        "n_steps_channel_selection: 3\n",
        "T: !apply:math.ceil\n",
        "    - !ref <sample_rate> * (<tmax> - <tmin>)\n",
        "C: 22\n",
        "test_with: 'best'\n",
        "test_key: \"acc\"\n",
        "\n",
        "# METRICS\n",
        "f1: !name:sklearn.metrics.f1_score\n",
        "    average: 'macro'\n",
        "acc: !name:sklearn.metrics.balanced_accuracy_score\n",
        "cm: !name:sklearn.metrics.confusion_matrix\n",
        "metrics:\n",
        "    f1: !ref <f1>\n",
        "    acc: !ref <acc>\n",
        "    cm: !ref <cm>\n",
        "\n",
        "# TRAINING HPARS\n",
        "n_train_examples: 100\n",
        "avg_models: 10\n",
        "max_lr: !ref <lr>\n",
        "base_lr: 0.00000001\n",
        "step_size_multiplier: 5\n",
        "step_size: !apply:round\n",
        "    - !ref <step_size_multiplier> * <n_train_examples> / <batch_size>\n",
        "lr_annealing: !new:speechbrain.nnet.schedulers.CyclicLRScheduler\n",
        "    base_lr: !ref <base_lr>\n",
        "    max_lr: !ref <max_lr>\n",
        "    step_size: !ref <step_size>\n",
        "label_smoothing: 0.0\n",
        "loss: !name:speechbrain.nnet.losses.nll_loss\n",
        "    label_smoothing: !ref <label_smoothing>\n",
        "optimizer: !name:torch.optim.Adam\n",
        "    lr: !ref <lr>\n",
        "epoch_counter: !new:speechbrain.utils.epoch_loop.EpochCounter\n",
        "    limit: !ref <number_of_epochs>\n",
        "batch_size: 32\n",
        "valid_ratio: 0.2\n",
        "\n",
        "# DATA NORMALIZATION\n",
        "dims_to_normalize: 1\n",
        "normalize: !name:speechbrain.processing.signal_processing.mean_std_norm\n",
        "    dims: !ref <dims_to_normalize>\n",
        "\n",
        "# MODEL\n",
        "input_shape: [null, !ref <T>, !ref <C>, null]\n",
        "cnn_temporal_kernels: 4\n",
        "cnn_temporal_kernelsize: 20\n",
        "cnn_spatial_depth_multiplier: 1\n",
        "cnn_spatial_pool: 2\n",
        "activation_type: 'relu'\n",
        "\n",
        "model: !new:models.SimpleEEGNet.SimpleEEGNet\n",
        "    input_shape: !ref <input_shape>\n",
        "    cnn_temporal_kernels: !ref <cnn_temporal_kernels>\n",
        "    cnn_temporal_kernelsize: !ref <cnn_temporal_kernelsize>\n",
        "    cnn_spatial_depth_multiplier: !ref <cnn_spatial_depth_multiplier>\n",
        "    cnn_spatial_pool: !ref <cnn_spatial_pool>\n",
        "    activation_type: !ref <activation_type>\n",
        "    dense_n_neurons: !ref <n_classes>\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "sample_hyperparams = tuned_hyperparams + other_hyperparams"
      ],
      "metadata": {
        "id": "D3CT9q0cIsvw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the yaml file on disk\n",
        "f = open('/content/sample_hyperparams.yaml', \"w\")\n",
        "f.write(sample_hyperparams)\n",
        "f.close()"
      ],
      "metadata": {
        "id": "W3jtgnlL9Aes"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/benchmarks/benchmarks/MOABB/\n",
        "\n",
        "!./run_hparam_optimization.sh --hparams '/content/sample_hyperparams.yaml' \\\n",
        "--data_folder '/content/data/BNCI2014001'\\\n",
        "--cached_data_folder '/content/data' \\\n",
        "--output_folder '/content/results/hyperparameter-search/BNCI2014001' \\\n",
        "--nsbj 9 --nsess 2 --nruns 1 --train_mode 'leave-one-session-out' \\\n",
        "--exp_name 'hyperparameter-search' \\\n",
        "--nsbj_hpsearch 1 --nsess_hpsearch 1 \\\n",
        "--nruns_eval 1 \\\n",
        "--eval_metric acc \\\n",
        "--exp_max_trials 5"
      ],
      "metadata": {
        "id": "o51NQCY29Bnh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Full Training"
      ],
      "metadata": {
        "id": "baPpqIKU30_Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Note: To run the model on the CPU, simply include --device='cpu' in the training command.\n",
        "!python train.py hparams/MotorImagery/BNCI2014001/SimpleEEGNet.yaml --data_folder=eeg_data --cached_data_folder=eeg_pickled_data --output_folder=results/MotorImagery/BNCI2014001/ --target_subject_idx=0 --target_session_idx=1 --data_iterator_name=leave-one-session-out"
      ],
      "metadata": {
        "id": "BnndWfGT4CGu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Observations"
      ],
      "metadata": {
        "id": "GzScPaS6DyRl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train the neural network on a single cross-validation fold takes 2minutes for EEGNet on CPU for EEGNet vs <br>\n",
        "Run a complete experiment by looping over the entire dataset takes 33minutes on CPU vs for EEGNet vs"
      ],
      "metadata": {
        "id": "c_6iLWasD2Z5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **About SpeechBrain**\n",
        "- Website: https://speechbrain.github.io/\n",
        "- Code: https://github.com/speechbrain/speechbrain/\n",
        "- HuggingFace: https://huggingface.co/speechbrain/\n",
        "\n",
        "\n",
        "# **Citing SpeechBrain**\n",
        "Please, cite SpeechBrain if you use it for your research or business.\n",
        "\n",
        "```bibtex\n",
        "@misc{speechbrain,\n",
        "  title={{SpeechBrain}: A General-Purpose Speech Toolkit},\n",
        "  author={Mirco Ravanelli and Titouan Parcollet and Peter Plantinga and Aku Rouhe and Samuele Cornell and Loren Lugosch and Cem Subakan and Nauman Dawalatabad and Abdelwahab Heba and Jianyuan Zhong and Ju-Chieh Chou and Sung-Lin Yeh and Szu-Wei Fu and Chien-Feng Liao and Elena Rastorgueva and Fran√ßois Grondin and William Aris and Hwidong Na and Yan Gao and Renato De Mori and Yoshua Bengio},\n",
        "  year={2021},\n",
        "  eprint={2106.04624},\n",
        "  archivePrefix={arXiv},\n",
        "  primaryClass={eess.AS},\n",
        "  note={arXiv:2106.04624}\n",
        "}\n",
        "```"
      ],
      "metadata": {
        "id": "5UN65pKF4StU"
      }
    }
  ]
}